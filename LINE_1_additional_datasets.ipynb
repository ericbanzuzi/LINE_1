{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LINE-1 IMPLEMENTATION\n",
    "\n",
    "The original source code from the paper can be found here: [https://github.com/tangjianpku/LINE/tree/master](https://github.com/tangjianpku/LINE/tree/master)\n",
    "\n",
    "This was also used slightly as help in this implementation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de462f04bfc665da"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:57:59.437451Z",
     "start_time": "2024-01-03T20:57:57.193646Z"
    }
   },
   "id": "efb8a0153e5b6b70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions\n",
    "\n",
    "These are the helper functions for LINE-1 algorithm. It uses an alias table that it samples from to make computations more efficient. Addtionally, it uses a negative sampling table for the same reasons.\n",
    "\n",
    "Moreover, the code used for evaluating embeddings in node classification and link prediction can be found in this section."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ebe2f6bc7c584"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def generate_alias(probs):\n",
    "    \"\"\"\n",
    "    Generates an Alias table to sample from based on some probability distribution p\n",
    "    :param probs: distribution to create the Alias table\n",
    "    :return: A (Alias table)\n",
    "    \"\"\"\n",
    "    l = len(probs)\n",
    "    L, H = [], []\n",
    "    \n",
    "    for i, p in enumerate(probs):\n",
    "        if p <= 1/l:\n",
    "            L.append((i, p))\n",
    "        else:\n",
    "            H.append((i, p))\n",
    "    \n",
    "    A = []\n",
    "    while len(L) > 0 and len(H) > 0:\n",
    "        i, p_i = L.pop()\n",
    "        h, p_h = H.pop()\n",
    "        \n",
    "        A.append((i, h, p_i))\n",
    "        if p_h - p_i > 1/l:\n",
    "            H.append((h, p_h - p_i))\n",
    "        else:\n",
    "            L.append((h, p_h - p_i))\n",
    "\n",
    "    # Handling any remaining items\n",
    "    while len(L) > 0:\n",
    "        i, p = L.pop()\n",
    "        A.append((i, i, 1/l))\n",
    "    \n",
    "    while len(H) > 0:\n",
    "        h, p = H.pop()\n",
    "        A.append((h, h, 1/l))\n",
    "    return A\n",
    "\n",
    "\n",
    "def sample_alias(A, l):\n",
    "    \"\"\"\n",
    "    Samples an outcome from the alias table over a distribution p\n",
    "    :param A: Alias table\n",
    "    :param l: length of the probability distribution\n",
    "    :return: an index based on the sampling outcome\n",
    "    \"\"\"\n",
    "    draw = np.random.randint(l)\n",
    "    i, h, p = A[draw]\n",
    "    if np.random.rand() < l * p:\n",
    "        return h\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "\n",
    "def init_neg_table(G, node2int, size):\n",
    "    \"\"\"\n",
    "    Creates a table for the negative sampling of vertices according to vertex degrees \n",
    "    :param G: Graph from which to sample\n",
    "    :param size: size of the table\n",
    "    :return: Negative sampling table\n",
    "    \"\"\"\n",
    "    degrees_prob = np.zeros(G.number_of_nodes())\n",
    "    for node, d in G.degree:\n",
    "        # P_n(v) ∝ d_v ** 0.75 from the paper\n",
    "        degrees_prob[node2int[node]] = d ** 0.75\n",
    "    \n",
    "    Z = np.sum(degrees_prob)\n",
    "    neg_table = np.zeros(int(size), dtype=np.uint32)\n",
    "    \n",
    "    neg_table_id = 0\n",
    "    for i, p in enumerate(degrees_prob):\n",
    "        portion_to_fill = np.round((p/Z) * size).astype(int)\n",
    "        neg_table[neg_table_id:neg_table_id+portion_to_fill] = i\n",
    "        neg_table_id += portion_to_fill\n",
    "    return neg_table\n",
    "\n",
    "\n",
    "# ---- PREDICTION TASKS ----\n",
    "def node_classification(X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    5-fold multi-label classification using one-vs-rest logistic regression\n",
    "    :param X: source embeddings of a graph\n",
    "    :param y: the labels for each node\n",
    "    :param n_folds: number of folds for cross-validation\n",
    "    :return: accuracy, f1 macro score, f1 micro score\n",
    "    \"\"\"\n",
    "    model = LogisticRegression()\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    eval_scores = {'acc': 'accuracy', 'f1_macro': 'f1_macro', 'f1_micro': 'f1_micro'}\n",
    "    results = cross_validate(ovr_model, X, y, cv=kf, scoring=eval_scores)\n",
    "    acc, f1_macro, f1_micro = results['test_acc'].mean(), results['test_f1_macro'].mean(), results['test_f1_micro'].mean()\n",
    "    return acc, f1_macro, f1_micro\n",
    "\n",
    "\n",
    "def predict_link(u, v, embeddings):\n",
    "    \"\"\"\n",
    "    Computes the normalized probability for an existing link between two nodes u and v based on the input\n",
    "    embeddings.\n",
    "    :param u: a node in the graph\n",
    "    :param v: a node in the graph\n",
    "    :param embeddings: trained embeddings\n",
    "    :return: sigmoid normalized probability for the existence of a link\n",
    "    \"\"\"\n",
    "    embedding1 = embeddings[u]\n",
    "    embedding2 = embeddings[v]\n",
    "    \n",
    "    # Compute inner product (dot product)\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "\n",
    "    # Normalize by sigmoid function\n",
    "    link_probability = 1/(1 + np.exp(-dot_product))\n",
    "    return link_probability\n",
    "\n",
    "\n",
    "def link_predictions(embeddings, edges, y_true):\n",
    "    \"\"\"\n",
    "    Computes the ROC-AUC score for a given set of test edges based on the trained embeddings.\n",
    "    :param embeddings: a models trained embeddings\n",
    "    :param edges: test edges\n",
    "    :param y_true: the labels for edges (1=true, 0=false)\n",
    "    :return: the ROC-AUC score from predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for edge in edges:\n",
    "        predictions.append(predict_link(edge[0], edge[1], embeddings))\n",
    "    return roc_auc_score(y_true, predictions) \n",
    "\n",
    "\n",
    "# reference: https://zqfang.github.io/2021-08-12-graph-linkpredict/\n",
    "def train_test_split_graph(G):\n",
    "    \"\"\"\n",
    "    Splits a Graph into a test and train set randomly to 80-20. The test split is balanced with negative edges sampled from random vertex pairs that have no edges between them. \n",
    "    While removing edges randomly, it makes sure that no vertex is isolated.\n",
    "    :param G: a networkx graph to be split\n",
    "    :return: the train-test split as torch geometrics graphs\n",
    "    \"\"\"\n",
    "    data = from_networkx(G)\n",
    "    try:\n",
    "        data.y = data.group_belonging\n",
    "    except:\n",
    "        try:\n",
    "            data.y = data.club  # this only happens with karate club\n",
    "        except:\n",
    "            a = 0  # this only happens for PPI, for link prediction we don't care about labels\n",
    "    data.x = torch.arange(G.number_of_nodes()).unsqueeze(1)\n",
    "    \n",
    "    transform = RandomLinkSplit(num_val=0, num_test=0.5, is_undirected=True, add_negative_train_samples=False)\n",
    "    train_data, _, test_data = transform(data)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# SILVIA'S METHOD\n",
    "def multi_label_classification(X, y, n_folds=5):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    micro_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Predictions\n",
    "        model = OneVsRestClassifier(LogisticRegression())\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        micro_f1_scores.append(micro_f1)\n",
    "        macro_f1_scores.append(macro_f1)\n",
    "    \n",
    "    # Calculate mean scores over all folds\n",
    "    mean_micro_f1 = np.mean(micro_f1_scores)\n",
    "    mean_macro_f1 = np.mean(macro_f1_scores)\n",
    "\n",
    "    return mean_micro_f1, mean_macro_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:31:37.479356Z",
     "start_time": "2024-01-05T16:31:37.463985Z"
    }
   },
   "id": "aa57d1b922649fc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LINE-1 class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39d2d3e322422ab7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Line:\n",
    "    def __init__(self, G, dim, alpha, neg_samples, num_edge_samples):\n",
    "        \"\"\"\n",
    "        Initializes a LINE-1 embedding algorithm\n",
    "        :param G: Graph as a networkx object\n",
    "        :param dim: dimension of embeddings\n",
    "        :param alpha: learning rate\n",
    "        :param neg_samples: number of negative samples in training\n",
    "        :param num_edge_samples: number of edge samples used in training\n",
    "        \"\"\"\n",
    "        self.neg_table_size = 1e8  # default, same as in the original source code\n",
    "        self.num_nodes = G.number_of_nodes()\n",
    "        self.num_edges = G.number_of_edges()\n",
    "        self.node2int = {node: i for i, node in enumerate(G.nodes())}  # dictionary for easy access in arrays\n",
    "        \n",
    "        # Compute the probability weights for each edge\n",
    "        self.edges = [[self.node2int[u], self.node2int[v]] for u, v in G.edges()]\n",
    "        edges_prob = np.array([G[u][v].get(\"weight\", 1) for u, v in G.edges()])\n",
    "        self.edges_prob = edges_prob/np.sum(edges_prob)\n",
    "        \n",
    "        # Create sampling tables for the algorithm\n",
    "        self.neg_table = init_neg_table(G, self.node2int, self.neg_table_size)\n",
    "        self.alias_table = generate_alias(self.edges_prob)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.dim = dim\n",
    "        self.alpha_0 = alpha\n",
    "        self.neg_samples = neg_samples\n",
    "        self.T = num_edge_samples\n",
    "        \n",
    "        self.embeddings = (np.random.random((self.num_nodes, self.dim)) - 0.5)/self.dim\n",
    "        \n",
    "    def update(self, u_i, u_j, alpha, acc_gradient, local_label):\n",
    "        \"\"\"\n",
    "        The update equation for the SGD in the LINE-1 algorithm\n",
    "        :param u_i: source representation for vertex i\n",
    "        :param u_j: source representation for vertex j\n",
    "        :param acc_gradient: accumulated gradient\n",
    "        :param local_label: label in the local graph (1=connected, 0=not connected)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        f_line = 1/(1 + np.exp(-(u_i @ u_j)))  # sigmoid\n",
    "        g = (local_label - f_line) * alpha\n",
    "        \n",
    "        acc_gradient += g * u_j\n",
    "        u_j += g * u_i\n",
    "        return \n",
    "\n",
    "    def train_line(self, file_name):\n",
    "        \"\"\"\n",
    "        Trains a LINE-1 algorithm embeddings via Edge Sampling with a minibatch size of 1\n",
    "        :return: trained embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        start = time.time()\n",
    "        alpha = self.alpha_0\n",
    "        for i in tqdm(range(int(self.T))):\n",
    "            if i > 0:\n",
    "                alpha = self.alpha_0*(1-i/self.T)\n",
    "            \n",
    "            edge = sample_alias(self.alias_table, self.num_edges)\n",
    "            u, v = self.edges[edge]\n",
    "            \n",
    "            acc_gradient = np.zeros(self.dim)\n",
    "            self.update(self.embeddings[u], self.embeddings[v], alpha, acc_gradient, local_label=1)\n",
    "            \n",
    "            for k in range(self.neg_samples):\n",
    "                v = random.choice(self.neg_table)\n",
    "                self.update(self.embeddings[u], self.embeddings[v], alpha, acc_gradient, local_label=0)\n",
    "            \n",
    "            self.embeddings[u] += acc_gradient\n",
    "\n",
    "        norm_trained_embeddings = normalize(self.embeddings)\n",
    "        np.save(f'./embeddings/{file_name}.npy', norm_trained_embeddings)\n",
    "        self.embeddings = norm_trained_embeddings\n",
    "        print(f\"Training time: {time.time() - start}s\")\n",
    "        return norm_trained_embeddings\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:58:06.780546Z",
     "start_time": "2024-01-03T20:58:06.770879Z"
    }
   },
   "id": "9dee4ce5975d55c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Amazon Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e78ce6b5c7ae0559"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/yandex-research/heterophilous-graphs/raw/main/data/amazon_ratings.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import HeterophilousGraphDataset\n",
    "\n",
    "dataset = HeterophilousGraphDataset(root=\"./\", name='amazon_ratings')\n",
    "data = dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:58:45.667598Z",
     "start_time": "2024-01-03T20:58:38.520181Z"
    }
   },
   "id": "5f991b676196fcc5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 24492  |  Number of edges: 93050\n"
     ]
    }
   ],
   "source": [
    "# convert to networkx\n",
    "G_amazon = to_networkx(data)\n",
    "G_amazon = G_amazon.to_undirected()\n",
    "\n",
    "# Create a dictionary to store groups for each ID\n",
    "group_dict = {}\n",
    "\n",
    "# Populate the group_dict\n",
    "for i in range(len(data.y)):\n",
    "    user_id = i\n",
    "    group_id = int(data.y[i].numpy())\n",
    "\n",
    "    # Check if the user_id is already in the dictionary\n",
    "    if user_id in group_dict:\n",
    "        if type(group_id) == int:\n",
    "            group_dict[user_id].append(group_id)\n",
    "        else:\n",
    "            group_dict[user_id].extend(group_id)\n",
    "    else:\n",
    "        if type(group_id) == int:\n",
    "            group_dict[user_id] = [group_id]\n",
    "        else:\n",
    "             group_dict[user_id] = list(group_id)\n",
    "\n",
    "# Add group labels to the nodes\n",
    "for user_id, groups in group_dict.items():\n",
    "    nx.set_node_attributes(G_amazon, {user_id: groups}, 'group_belonging')\n",
    "\n",
    "# Print basic graph information\n",
    "print(\"Number of nodes:\", G_amazon.number_of_nodes(), ' | ', \"Number of edges:\", G_amazon.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:59:02.170311Z",
     "start_time": "2024-01-03T20:59:01.303231Z"
    }
   },
   "id": "fda354199576132f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Find and preprocess labels for the graph\n",
    "labels = []\n",
    "for n in G_amazon.nodes:\n",
    "    l = G_amazon.nodes[n].get('group_belonging')\n",
    "    labels.append(l)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "preprocessed_labels = mlb.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T08:34:43.254835Z",
     "start_time": "2024-01-04T08:34:43.200522Z"
    }
   },
   "id": "c438000578d6eced"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# hyperparameters from the paper\n",
    "dim = 128\n",
    "alpha = 0.025\n",
    "neg_samples = 5\n",
    "T = 1e9\n",
    "\n",
    "model_amazon = Line(G_amazon, dim, alpha, neg_samples, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:59:21.075448Z",
     "start_time": "2024-01-03T20:59:20.562260Z"
    }
   },
   "id": "a4ee0652de4e4704"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000000/1000000000 [6:49:47<00:00, 40671.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 24587.51318192482s\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_amazon = model_amazon.train_line('amazon')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T03:49:20.492870Z",
     "start_time": "2024-01-03T20:59:32.610331Z"
    }
   },
   "id": "f22db2b1ae402a40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings_model_amazon = np.load('./embeddings/amazon.npy')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98206664a4e670ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Node Classification:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58062839e519031"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- F1 macro: 0.092408263651144 F1 micro: 0.14911039615510685\n",
      "Shuffle 2 -- F1 macro: 0.09194528471140632 F1 micro: 0.14811836738979683\n",
      "Shuffle 3 -- F1 macro: 0.0929988912966155 F1 micro: 0.1498381269408421\n",
      "Shuffle 4 -- F1 macro: 0.09401994590940693 F1 micro: 0.1518425905670805\n",
      "Shuffle 5 -- F1 macro: 0.09357582325125047 F1 micro: 0.1508539230035068\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f1_macros, f1_micros = np.zeros(N), np.zeros(N)\n",
    "for i in range(N):\n",
    "    _, f1_macro, f1_micro = node_classification(embeddings_model_amazon, preprocessed_labels)\n",
    "    f1_macros[i] = f1_macro\n",
    "    f1_micros[i] = f1_micro\n",
    "    print(f'Shuffle {i+1} --', 'F1 macro:', f1_macro, 'F1 micro:', f1_micro)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T08:35:10.604223Z",
     "start_time": "2024-01-04T08:34:55.130493Z"
    }
   },
   "id": "7d6c184b436069fc"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "F1 macro: 0.09298964176396465 F1 micro: 0.14995268081126661\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('F1 macro:', np.mean(f1_macros), 'F1 micro:', np.mean(f1_micros))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T08:35:14.468830Z",
     "start_time": "2024-01-04T08:35:14.427902Z"
    }
   },
   "id": "5f3325ce27b39ed2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Link Prediction:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3feb3bf2b13cde27"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- ROC-AUC: 0.9611885590737907\n",
      "Shuffle 2 -- ROC-AUC: 0.9620543352464297\n",
      "Shuffle 3 -- ROC-AUC: 0.961853190969015\n",
      "Shuffle 4 -- ROC-AUC: 0.9608228730747167\n",
      "Shuffle 5 -- ROC-AUC: 0.9623520749015179\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "roc_auc_scores = np.zeros(N)\n",
    "for i in range(N):\n",
    "    train_data, test_data = train_test_split_graph(G_amazon)\n",
    "\n",
    "    # Prepare edges\n",
    "    test_edges = test_data.edge_label_index.numpy().T\n",
    "    y_true = test_data.edge_label.numpy()\n",
    "\n",
    "    score = link_predictions(embeddings_model_amazon, test_edges, y_true)\n",
    "    roc_auc_scores[i] = score\n",
    "    print(f'Shuffle {i+1} --', 'ROC-AUC:', score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T08:00:30.013226Z",
     "start_time": "2024-01-05T08:00:20.824632Z"
    }
   },
   "id": "3c938b3bd65d6929"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "ROC-AUC: 0.961654206653094\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('ROC-AUC:', np.mean(roc_auc_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T08:00:30.018655Z",
     "start_time": "2024-01-05T08:00:30.012318Z"
    }
   },
   "id": "bbdb54f7e40ac86b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PPI Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82c439d219df27d8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting ./ppi.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "dataset = PPI(\"./\")\n",
    "data = dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T08:07:51.854236Z",
     "start_time": "2024-01-05T08:07:44.449746Z"
    }
   },
   "id": "8605f46123077ba8"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1767  |  Number of edges: 16159\n"
     ]
    }
   ],
   "source": [
    "# convert to networkx\n",
    "G_ppi = to_networkx(data)\n",
    "G_ppi = G_ppi.to_undirected()\n",
    "    \n",
    "# for this model the labels are already multi-label binarized, so no need for pre-processing\n",
    "preprocessed_labels = data.y.numpy()\n",
    "\n",
    "# Print basic graph information\n",
    "print(\"Number of nodes:\", G_ppi.number_of_nodes(), ' | ', \"Number of edges:\", G_ppi.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T08:44:05.946038Z",
     "start_time": "2024-01-05T08:44:05.913329Z"
    }
   },
   "id": "483d2a717ad42645"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# hyperparameters from the paper\n",
    "dim = 128\n",
    "alpha = 0.025\n",
    "neg_samples = 5\n",
    "T = 1e9\n",
    "\n",
    "model_ppi = Line(G_ppi, dim, alpha, neg_samples, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T08:44:34.516817Z",
     "start_time": "2024-01-05T08:44:34.418057Z"
    }
   },
   "id": "3caf14bc91c23f77"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000000/1000000000 [7:13:39<00:00, 38431.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 26020.00948691368s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_ppi = model_ppi.train_line('ppi')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:02:25.712237Z",
     "start_time": "2024-01-05T08:48:45.696564Z"
    }
   },
   "id": "56a7e72076033dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings_model_ppi = np.load('./embeddings/ppi.npy')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a077e4ca0ef1f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Node Classification:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88176df8323d35eb"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- F1 macro: 0.27299352474108163 F1 micro: 0.4712210419365386\n",
      "Shuffle 2 -- F1 macro: 0.2762610732182835 F1 micro: 0.4737800229274175\n",
      "Shuffle 3 -- F1 macro: 0.27937880250763314 F1 micro: 0.47655230966114503\n",
      "Shuffle 4 -- F1 macro: 0.28324026198569147 F1 micro: 0.4812089240398437\n",
      "Shuffle 5 -- F1 macro: 0.27794462411085785 F1 micro: 0.47535791404734445\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f1_macros, f1_micros = np.zeros(N), np.zeros(N)\n",
    "for i in range(N):\n",
    "    _, f1_macro, f1_micro = node_classification(embeddings_model_ppi, preprocessed_labels)\n",
    "    f1_macros[i] = f1_macro\n",
    "    f1_micros[i] = f1_micro\n",
    "    print(f'Shuffle {i+1} --', 'F1 macro:', f1_macro, 'F1 micro:', f1_micro)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:28:27.690947Z",
     "start_time": "2024-01-05T16:27:58.870911Z"
    }
   },
   "id": "ab51ef75bd1436ae"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "F1 macro: 0.2779636573127095 F1 micro: 0.4756240425224578\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('F1 macro:', np.mean(f1_macros), 'F1 micro:', np.mean(f1_micros))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:28:27.774058Z",
     "start_time": "2024-01-05T16:28:27.718361Z"
    }
   },
   "id": "ad345f8cab0514c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Link Prediction:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e431d3261ff4d6bb"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- ROC-AUC: 0.9376663708044222\n",
      "Shuffle 2 -- ROC-AUC: 0.936006211467796\n",
      "Shuffle 3 -- ROC-AUC: 0.9322959141517495\n",
      "Shuffle 4 -- ROC-AUC: 0.9352196355457\n",
      "Shuffle 5 -- ROC-AUC: 0.936095900120853\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "roc_auc_scores = np.zeros(N)\n",
    "for i in range(N):\n",
    "    train_data, test_data = train_test_split_graph(G_ppi)\n",
    "\n",
    "    # Prepare edges\n",
    "    test_edges = test_data.edge_label_index.numpy().T\n",
    "    y_true = test_data.edge_label.numpy()\n",
    "\n",
    "    score = link_predictions(embeddings_model_ppi, test_edges, y_true)\n",
    "    roc_auc_scores[i] = score\n",
    "    print(f'Shuffle {i+1} --', 'ROC-AUC:', score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:31:48.110861Z",
     "start_time": "2024-01-05T16:31:46.823672Z"
    }
   },
   "id": "df5fc8eee449a0f1"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "ROC-AUC: 0.9354568064181041\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('ROC-AUC:', np.mean(roc_auc_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:31:48.647323Z",
     "start_time": "2024-01-05T16:31:48.606895Z"
    }
   },
   "id": "e15e8c80cfed4e97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bac96ca8ccae03c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
