{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LINE-1 IMPLEMENTATION\n",
    "\n",
    "The original source code from the paper can be found here: [https://github.com/tangjianpku/LINE/tree/master](https://github.com/tangjianpku/LINE/tree/master)\n",
    "\n",
    "This was also used slightly as help in this implementation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc21d678060021b4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from torch_geometric.transforms import RandomLinkSplit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T09:16:52.754738Z",
     "start_time": "2024-01-06T09:16:50.804661Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions\n",
    "\n",
    "These are the helper functions for LINE-1 algorithm. It uses an alias table that it samples from to make computations more efficient. Addtionally, it uses a negative sampling table for the same reasons.\n",
    "\n",
    "Moreover, the code used for evaluating embeddings in node classification and link prediction can be found in this section."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f1cc4f0e085226"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def generate_alias(probs):\n",
    "    \"\"\"\n",
    "    Generates an Alias table to sample from based on some probability distribution p\n",
    "    :param probs: distribution to create the Alias table\n",
    "    :return: A (Alias table)\n",
    "    \"\"\"\n",
    "    l = len(probs)\n",
    "    L, H = [], []\n",
    "    \n",
    "    for i, p in enumerate(probs):\n",
    "        if p <= 1/l:\n",
    "            L.append((i, p))\n",
    "        else:\n",
    "            H.append((i, p))\n",
    "    \n",
    "    A = []\n",
    "    while len(L) > 0 and len(H) > 0:\n",
    "        i, p_i = L.pop()\n",
    "        h, p_h = H.pop()\n",
    "        \n",
    "        A.append((i, h, p_i))\n",
    "        if p_h - p_i > 1/l:\n",
    "            H.append((h, p_h - p_i))\n",
    "        else:\n",
    "            L.append((h, p_h - p_i))\n",
    "\n",
    "    # Handling any remaining items\n",
    "    while len(L) > 0:\n",
    "        i, p = L.pop()\n",
    "        A.append((i, i, 1/l))\n",
    "    \n",
    "    while len(H) > 0:\n",
    "        h, p = H.pop()\n",
    "        A.append((h, h, 1/l))\n",
    "    return A\n",
    "\n",
    "\n",
    "def sample_alias(A, l):\n",
    "    \"\"\"\n",
    "    Samples an outcome from the alias table over a distribution p\n",
    "    :param A: Alias table\n",
    "    :param l: length of the probability distribution\n",
    "    :return: an index based on the sampling outcome\n",
    "    \"\"\"\n",
    "    draw = np.random.randint(l)\n",
    "    i, h, p = A[draw]\n",
    "    if np.random.rand() < l * p:\n",
    "        return h\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "\n",
    "def init_neg_table(G, node2int, size):\n",
    "    \"\"\"\n",
    "    Creates a table for the negative sampling of vertices according to vertex degrees \n",
    "    :param G: Graph from which to sample\n",
    "    :param size: size of the table\n",
    "    :return: Negative sampling table\n",
    "    \"\"\"\n",
    "    degrees_prob = np.zeros(G.number_of_nodes())\n",
    "    for node, d in G.degree:\n",
    "        # P_n(v) ∝ d_v ** 0.75 from the paper\n",
    "        degrees_prob[node2int[node]] = d ** 0.75\n",
    "    \n",
    "    Z = np.sum(degrees_prob)\n",
    "    neg_table = np.zeros(int(size), dtype=np.uint32)\n",
    "    \n",
    "    neg_table_id = 0\n",
    "    for i, p in enumerate(degrees_prob):\n",
    "        portion_to_fill = np.round((p/Z) * size).astype(int)\n",
    "        neg_table[neg_table_id:neg_table_id+portion_to_fill] = i\n",
    "        neg_table_id += portion_to_fill\n",
    "    return neg_table\n",
    "\n",
    "\n",
    "# ---- PREDICTION TASKS ----\n",
    "def node_classification(X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    5-fold multi-label classification using one-vs-rest logistic regression\n",
    "    :param X: source embeddings of a graph\n",
    "    :param y: the labels for each node\n",
    "    :param n_folds: number of folds for cross-validation\n",
    "    :return: accuracy, f1 macro score, f1 micro score\n",
    "    \"\"\"\n",
    "    model = LogisticRegression()\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    eval_scores = {'acc': 'accuracy', 'f1_macro': 'f1_macro', 'f1_micro': 'f1_micro'}\n",
    "    results = cross_validate(ovr_model, X, y, cv=kf, scoring=eval_scores)\n",
    "    acc, f1_macro, f1_micro = results['test_acc'].mean(), results['test_f1_macro'].mean(), results['test_f1_micro'].mean()\n",
    "    return acc, f1_macro, f1_micro\n",
    "\n",
    "\n",
    "def predict_link(u, v, embeddings):\n",
    "    \"\"\"\n",
    "    Computes the normalized probability for an existing link between two nodes u and v based on the input\n",
    "    embeddings.\n",
    "    :param u: a node in the graph\n",
    "    :param v: a node in the graph\n",
    "    :param embeddings: trained embeddings\n",
    "    :return: sigmoid normalized probability for the existence of a link\n",
    "    \"\"\"\n",
    "    embedding1 = embeddings[u]\n",
    "    embedding2 = embeddings[v]\n",
    "    \n",
    "    # Compute inner product (dot product)\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "\n",
    "    # Normalize by sigmoid function\n",
    "    link_probability = 1/(1 + np.exp(-dot_product))\n",
    "    return link_probability\n",
    "\n",
    "\n",
    "def link_predictions(embeddings, edges, y_true):\n",
    "    \"\"\"\n",
    "    Computes the ROC-AUC score for a given set of test edges based on the trained embeddings.\n",
    "    :param embeddings: a models trained embeddings\n",
    "    :param edges: test edges\n",
    "    :param y_true: the labels for edges (1=true, 0=false)\n",
    "    :return: the ROC-AUC score from predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for edge in edges:\n",
    "        predictions.append(predict_link(edge[0], edge[1], embeddings))\n",
    "    return roc_auc_score(y_true, predictions) \n",
    "\n",
    "\n",
    "# reference: https://zqfang.github.io/2021-08-12-graph-linkpredict/\n",
    "def train_test_split_graph(G):\n",
    "    \"\"\"\n",
    "    Splits a Graph into a test and train set randomly to 80-20. The test split is balanced with negative edges sampled from random vertex pairs that have no edges between them. \n",
    "    While removing edges randomly, it makes sure that no vertex is isolated.\n",
    "    :param G: a networkx graph to be split\n",
    "    :return: the train-test split as torch geometrics graphs\n",
    "    \"\"\"\n",
    "    data = from_networkx(G)\n",
    "    try:\n",
    "        data.y = data.group_belonging\n",
    "    except:\n",
    "        try:\n",
    "            data.y = data.club  # this only happens with karate club\n",
    "        except:\n",
    "            print('No labels')  # only for youtube\n",
    "    data.x = torch.arange(G.number_of_nodes()).unsqueeze(1)\n",
    "    \n",
    "    transform = RandomLinkSplit(num_val=0, num_test=0.5, is_undirected=True, add_negative_train_samples=False)\n",
    "    train_data, _, test_data = transform(data)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# SILVIA'S METHOD\n",
    "def multi_label_classification(X, y, n_folds=5):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    micro_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Predictions\n",
    "        model = OneVsRestClassifier(LogisticRegression())\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        micro_f1_scores.append(micro_f1)\n",
    "        macro_f1_scores.append(macro_f1)\n",
    "    \n",
    "    # Calculate mean scores over all folds\n",
    "    mean_micro_f1 = np.mean(micro_f1_scores)\n",
    "    mean_macro_f1 = np.mean(macro_f1_scores)\n",
    "\n",
    "    return mean_micro_f1, mean_macro_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T09:16:52.876679Z",
     "start_time": "2024-01-06T09:16:52.862694Z"
    }
   },
   "id": "bbecf4948717e105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LINE-1 class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf02377a97514a9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Line:\n",
    "    def __init__(self, G, dim, alpha, neg_samples, num_edge_samples):\n",
    "        \"\"\"\n",
    "        Initializes a LINE-1 embedding algorithm\n",
    "        :param G: Graph as a networkx object\n",
    "        :param dim: dimension of embeddings\n",
    "        :param alpha: learning rate\n",
    "        :param neg_samples: number of negative samples in training\n",
    "        :param num_edge_samples: number of edge samples used in training\n",
    "        \"\"\"\n",
    "        self.neg_table_size = 1e8  # default, same as in the original source code\n",
    "        self.num_nodes = G.number_of_nodes()\n",
    "        self.num_edges = G.number_of_edges()\n",
    "        self.node2int = {node: i for i, node in enumerate(G.nodes())}  # dictionary for easy access in arrays\n",
    "        \n",
    "        # Compute the probability weights for each edge\n",
    "        self.edges = [[self.node2int[u], self.node2int[v]] for u, v in G.edges()]\n",
    "        edges_prob = np.array([G[u][v].get(\"weight\", 1) for u, v in G.edges()])\n",
    "        self.edges_prob = edges_prob/np.sum(edges_prob)\n",
    "        \n",
    "        # Create sampling tables for the algorithm\n",
    "        self.neg_table = init_neg_table(G, self.node2int, self.neg_table_size)\n",
    "        self.alias_table = generate_alias(self.edges_prob)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.dim = dim\n",
    "        self.alpha_0 = alpha\n",
    "        self.neg_samples = neg_samples\n",
    "        self.T = num_edge_samples\n",
    "        \n",
    "        self.embeddings = (np.random.random((self.num_nodes, self.dim)) - 0.5)/self.dim\n",
    "        \n",
    "    def update(self, u_i, u_j, alpha, acc_gradient, local_label):\n",
    "        \"\"\"\n",
    "        The update equation for the SGD in the LINE-1 algorithm\n",
    "        :param u_i: source representation for vertex i\n",
    "        :param u_j: source representation for vertex j\n",
    "        :param acc_gradient: accumulated gradient\n",
    "        :param local_label: label in the local graph (1=connected, 0=not connected)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        f_line = 1/(1 + np.exp(-(u_i @ u_j)))  # sigmoid\n",
    "        g = (local_label - f_line) * alpha\n",
    "        \n",
    "        acc_gradient += g * u_j\n",
    "        u_j += g * u_i\n",
    "        return \n",
    "\n",
    "    def train_line(self, file_name):\n",
    "        \"\"\"\n",
    "        Trains a LINE-1 algorithm embeddings via Edge Sampling with a minibatch size of 1\n",
    "        :return: trained embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        start = time.time()\n",
    "        alpha = self.alpha_0\n",
    "        for i in tqdm(range(int(self.T))):\n",
    "            if i > 0:\n",
    "                alpha = self.alpha_0*(1-i/self.T)\n",
    "            \n",
    "            edge = sample_alias(self.alias_table, self.num_edges)\n",
    "            u, v = self.edges[edge]\n",
    "            \n",
    "            acc_gradient = np.zeros(self.dim)\n",
    "            self.update(self.embeddings[u], self.embeddings[v], alpha, acc_gradient, local_label=1)\n",
    "            \n",
    "            for k in range(self.neg_samples):\n",
    "                v = random.choice(self.neg_table)\n",
    "                self.update(self.embeddings[u], self.embeddings[v], alpha, acc_gradient, local_label=0)\n",
    "            \n",
    "            self.embeddings[u] += acc_gradient\n",
    "\n",
    "        norm_trained_embeddings = normalize(self.embeddings)\n",
    "        np.save(f'./embeddings/{file_name}.npy', norm_trained_embeddings)\n",
    "        self.embeddings = norm_trained_embeddings\n",
    "        print(f\"Training time: {time.time() - start}s\")\n",
    "        return norm_trained_embeddings\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T09:16:59.589362Z",
     "start_time": "2024-01-06T09:16:59.563566Z"
    }
   },
   "id": "9b1a939954d19010"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flickr Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61da2b3284103a24"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "edges_path = './Flickr-dataset/data/edges.csv'\n",
    "nodes_path = './Flickr-dataset/data/nodes.csv'\n",
    "groups_path = './Flickr-dataset/data/groups.csv'\n",
    "group_edges_path = './Flickr-dataset/data/group-edges.csv'\n",
    "\n",
    "nodes_id = pd.read_csv(nodes_path, header=None, names=['id'])\n",
    "groups_id = pd.read_csv(groups_path, header=None, names=['group'])\n",
    "edges = pd.read_csv(edges_path, header=None, names=['id_1', 'id_2'])\n",
    "user_group_membership = pd.read_csv(group_edges_path, header=None, names=['id', 'group'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:15:55.225488Z",
     "start_time": "2024-01-02T16:15:54.791520Z"
    }
   },
   "id": "e5543ff4fb664919"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G_flickr = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "G_flickr.add_nodes_from(nodes_id['id'])\n",
    "\n",
    "# Add edges to the graph\n",
    "G_flickr.add_edges_from(edges[['id_1', 'id_2']].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:16:05.014316Z",
     "start_time": "2024-01-02T16:15:58.056835Z"
    }
   },
   "id": "31ddfdbf4449d2df"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 80513  |  Number of edges: 5899882\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store groups for each ID\n",
    "group_dict = {}\n",
    "\n",
    "# Populate the group_dict\n",
    "for _, row in user_group_membership.iterrows():\n",
    "    user_id = row['id']\n",
    "    group_id = row['group']\n",
    "\n",
    "    # Check if the user_id is already in the dictionary\n",
    "    if user_id in group_dict:\n",
    "        group_dict[user_id].append(group_id)\n",
    "    else:\n",
    "        group_dict[user_id] = [group_id]\n",
    "\n",
    "# Add group labels to the nodes\n",
    "for user_id, groups in group_dict.items():\n",
    "    nx.set_node_attributes(G_flickr, {user_id: groups}, 'group_belonging')\n",
    "\n",
    "# Print basic graph information\n",
    "print(\"Number of nodes:\", G_flickr.number_of_nodes(), ' | ', \"Number of edges:\", G_flickr.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:16:20.884574Z",
     "start_time": "2024-01-02T16:16:18.494094Z"
    }
   },
   "id": "7763c185a5acf523"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Find and preprocess labels for the graph\n",
    "labels = []\n",
    "for n in G_flickr.nodes:\n",
    "    l = G_flickr.nodes[n].get('group_belonging')\n",
    "    labels.append(l)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "preprocessed_labels = mlb.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:16:22.684469Z",
     "start_time": "2024-01-02T16:16:22.597457Z"
    }
   },
   "id": "1f009c7a0b80287"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# hyperparameters from the paper\n",
    "dim = 128\n",
    "alpha = 0.025\n",
    "neg_samples = 5\n",
    "T = 1e9\n",
    "\n",
    "model_flickr = Line(G_flickr, dim, alpha, neg_samples, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:16:46.587267Z",
     "start_time": "2024-01-02T16:16:32.963693Z"
    }
   },
   "id": "5d7a12346ad9acc7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000000/1000000000 [7:26:15<00:00, 37347.26it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 26775.90695786476s\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_flickr = model_flickr.train_line('flickr')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T23:47:19.904528Z",
     "start_time": "2024-01-02T16:21:03.886608Z"
    }
   },
   "id": "d5422946e95c53ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_model_flickr = np.load('./embeddings/flickr.npy')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccc1031ca4cd68eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Node Classification:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dc57eb840d816f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- F1 macro: 0.1093505917150007 F1 micro: 0.24001922287709862\n",
      "Shuffle 2 -- F1 macro: 0.11024349410182673 F1 micro: 0.24049589540655142\n",
      "Shuffle 3 -- F1 macro: 0.11046383391444581 F1 micro: 0.24123100654959523\n",
      "Shuffle 4 -- F1 macro: 0.11025984517029346 F1 micro: 0.24116316021219864\n",
      "Shuffle 5 -- F1 macro: 0.1110781932341988 F1 micro: 0.24104611684485713\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f1_macros, f1_micros = np.zeros(N), np.zeros(N)\n",
    "for i in range(N):\n",
    "    _, f1_macro, f1_micro = node_classification(embeddings_model_flickr, preprocessed_labels)\n",
    "    f1_macros[i] = f1_macro\n",
    "    f1_micros[i] = f1_micro\n",
    "    print(f'Shuffle {i+1} --', 'F1 macro:', f1_macro, 'F1 micro:', f1_micro)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T00:25:15.188804Z",
     "start_time": "2024-01-02T23:47:46.538649Z"
    }
   },
   "id": "36eb8292f9cc2d60"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "F1 macro: 0.11027919162715309 F1 micro: 0.24079108037806024\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('F1 macro:', np.mean(f1_macros), 'F1 micro:', np.mean(f1_micros))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T00:25:16.704062Z",
     "start_time": "2024-01-03T00:25:15.232230Z"
    }
   },
   "id": "63fc7c20b0606488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Link Prediction:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7358d1eceeb61d5"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- ROC-AUC: 0.7757859136307986\n",
      "Shuffle 2 -- ROC-AUC: 0.7760684936323055\n",
      "Shuffle 3 -- ROC-AUC: 0.776027779504015\n",
      "Shuffle 4 -- ROC-AUC: 0.7760370577166799\n",
      "Shuffle 5 -- ROC-AUC: 0.775577489183476\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "roc_auc_scores = np.zeros(N)\n",
    "for i in range(N):\n",
    "    train_data, test_data = train_test_split_graph(G_flickr)\n",
    "\n",
    "    # Prepare edges\n",
    "    test_edges = test_data.edge_label_index.numpy().T\n",
    "    y_true = test_data.edge_label.numpy()\n",
    "\n",
    "    score = link_predictions(embeddings_model_flickr, test_edges, y_true)\n",
    "    roc_auc_scores[i] = score\n",
    "    print(f'Shuffle {i+1} --', 'ROC-AUC:', score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T22:32:04.837708Z",
     "start_time": "2024-01-04T22:23:12.528554Z"
    }
   },
   "id": "605611563baf302f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "ROC-AUC: 0.7758993467334551\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('ROC-AUC:', np.mean(roc_auc_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T22:32:04.868465Z",
     "start_time": "2024-01-04T22:32:04.852220Z"
    }
   },
   "id": "78899686e5774294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# YouTube Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a06c5b2cc49f5267"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "edges_path = './YouTube-dataset/data/edges.csv'\n",
    "nodes_path = './YouTube-dataset/data/nodes.csv'\n",
    "groups_path = './YouTube-dataset/data/groups.csv'\n",
    "group_edges_path = './YouTube-dataset/data/group-edges.csv'\n",
    "\n",
    "nodes_id = pd.read_csv(nodes_path, header=None, names=['id'])\n",
    "groups_id = pd.read_csv(groups_path, header=None, names=['group'])\n",
    "edges = pd.read_csv(edges_path, header=None, names=['id_1', 'id_2'])\n",
    "user_group_membership = pd.read_csv(group_edges_path, header=None, names=['id', 'group'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:26:41.463421Z",
     "start_time": "2024-01-03T12:26:41.187057Z"
    }
   },
   "id": "8aaccec1e455c4aa"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G_YT = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "G_YT.add_nodes_from(nodes_id['id'])\n",
    "\n",
    "# Add edges to the graph\n",
    "G_YT.add_edges_from(edges[['id_1', 'id_2']].values)\n",
    "\n",
    "# Create a copy of the graph for link prediction before adding labels, since some of the nodes don't have labels the method crashes otherwise\n",
    "G_YT2 = G_YT.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:26:51.142670Z",
     "start_time": "2024-01-03T12:26:46.667416Z"
    }
   },
   "id": "b09475aa806d8e21"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1138499  |  Number of edges: 2990443\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store groups for each ID\n",
    "group_dict = {}\n",
    "\n",
    "# Populate the group_dict\n",
    "for _, row in user_group_membership.iterrows():\n",
    "    user_id = row['id']\n",
    "    group_id = row['group']\n",
    "\n",
    "    # Check if the user_id is already in the dictionary\n",
    "    if user_id in group_dict:\n",
    "        group_dict[user_id].append(group_id)\n",
    "    else:\n",
    "        group_dict[user_id] = [group_id]\n",
    "\n",
    "# Add group labels to the nodes\n",
    "for user_id, groups in group_dict.items():\n",
    "    nx.set_node_attributes(G_YT, {user_id: groups}, 'group_belonging')\n",
    "\n",
    "# Print basic graph information\n",
    "print(\"Number of nodes:\", G_YT.number_of_nodes(), ' | ', \"Number of edges:\", G_YT.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:27:02.208922Z",
     "start_time": "2024-01-03T12:27:00.901026Z"
    }
   },
   "id": "65d99c54c77fccd6"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Find and preprocess labels for the graph, most nodes do not have labels so leave them out\n",
    "labels = []\n",
    "indices = []\n",
    "for i, n in enumerate(G_YT.nodes):\n",
    "    l = G_YT.nodes[n].get('group_belonging')\n",
    "    if l != None:\n",
    "        labels.append(l)\n",
    "        indices.append(i)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "preprocessed_labels = mlb.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:40:28.454831Z",
     "start_time": "2024-01-03T20:40:28.196110Z"
    }
   },
   "id": "9162c154733239d5"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# hyperparameters from the paper\n",
    "dim = 128\n",
    "alpha = 0.025\n",
    "neg_samples = 5\n",
    "T = 1e9\n",
    "\n",
    "model_yt = Line(G_YT, dim, alpha, neg_samples, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:28:23.595736Z",
     "start_time": "2024-01-03T12:28:15.307297Z"
    }
   },
   "id": "a43ff82406c0e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000000/1000000000 [7:40:37<00:00, 36182.69it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 27638.943197250366s\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_yt = model_yt.train_line('youtube')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:16:04.996975Z",
     "start_time": "2024-01-03T12:28:43.314436Z"
    }
   },
   "id": "1f8994a2c22c217d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_model_yt = np.load('./embeddings/youtube.npy')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8574bab92d26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Node Classification:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8614f991617d9fa"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- F1 macro: 0.21879560136393889 F1 micro: 0.2929703145996874\n",
      "Shuffle 2 -- F1 macro: 0.21841685041915698 F1 micro: 0.29284233497228035\n",
      "Shuffle 3 -- F1 macro: 0.2194178898023121 F1 micro: 0.2933688411087377\n",
      "Shuffle 4 -- F1 macro: 0.2184629801304176 F1 micro: 0.29334369387037584\n",
      "Shuffle 5 -- F1 macro: 0.21867739781644344 F1 micro: 0.2930418131645823\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f1_macros, f1_micros = np.zeros(N), np.zeros(N)\n",
    "for i in range(N):\n",
    "    _, f1_macro, f1_micro = node_classification(embeddings_model_yt[indices], preprocessed_labels)\n",
    "    f1_macros[i] = f1_macro\n",
    "    f1_micros[i] = f1_micro\n",
    "    print(f'Shuffle {i+1} --', 'F1 macro:', f1_macro, 'F1 micro:', f1_micro)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:48:13.128303Z",
     "start_time": "2024-01-03T20:41:08.707957Z"
    }
   },
   "id": "8630d502f3c83626"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "F1 macro: 0.2187541439064538 F1 micro: 0.2931133995431327\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('F1 macro:', np.mean(f1_macros), 'F1 micro:', np.mean(f1_micros))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:48:34.204582Z",
     "start_time": "2024-01-03T20:48:34.179151Z"
    }
   },
   "id": "e836076888c7dfdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Link Prediction:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba19610311ee0d6"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No labels\n",
      "Shuffle 1 -- ROC-AUC: 0.593335804968768\n",
      "No labels\n",
      "Shuffle 2 -- ROC-AUC: 0.593483231118844\n",
      "No labels\n",
      "Shuffle 3 -- ROC-AUC: 0.5939454527615484\n",
      "No labels\n",
      "Shuffle 4 -- ROC-AUC: 0.5932828620257373\n",
      "No labels\n",
      "Shuffle 5 -- ROC-AUC: 0.5936624478545048\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "roc_auc_scores = np.zeros(N)\n",
    "for i in range(N):\n",
    "    train_data, test_data = train_test_split_graph(G_YT2)\n",
    "\n",
    "    # Prepare edges\n",
    "    test_edges = test_data.edge_label_index.numpy().T\n",
    "    y_true = test_data.edge_label.numpy()\n",
    "\n",
    "    score = link_predictions(embeddings_model_yt, test_edges, y_true)\n",
    "    roc_auc_scores[i] = score\n",
    "    print(f'Shuffle {i+1} --', 'ROC-AUC:', score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T23:22:11.714090Z",
     "start_time": "2024-01-04T23:17:36.527201Z"
    }
   },
   "id": "2af49716d22b968a"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "ROC-AUC: 0.5935419597458805\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('ROC-AUC:', np.mean(roc_auc_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T23:22:11.728018Z",
     "start_time": "2024-01-04T23:22:11.723639Z"
    }
   },
   "id": "2972f04530e21c86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reddit Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a5b786b2b1efeb8"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Reddit\n",
    "dataset = Reddit(\"./\")\n",
    "data = dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T09:17:43.396812Z",
     "start_time": "2024-01-06T09:17:42.138938Z"
    }
   },
   "id": "f933e157e48d1930"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 232965  |  Number of edges: 57307946\n"
     ]
    }
   ],
   "source": [
    "# convert to networkx\n",
    "G_reddit = to_networkx(data)\n",
    "G_reddit = G_reddit.to_undirected()\n",
    "\n",
    "# Create a dictionary to store groups for each ID\n",
    "group_dict = {}\n",
    "\n",
    "# Populate the group_dict\n",
    "for i in range(len(data.y)):\n",
    "    user_id = i\n",
    "    group_id = int(data.y[i].numpy())\n",
    "\n",
    "    # Check if the user_id is already in the dictionary\n",
    "    if user_id in group_dict:\n",
    "        if type(group_id) == int:\n",
    "            group_dict[user_id].append(group_id)\n",
    "        else:\n",
    "            group_dict[user_id].extend(group_id)\n",
    "    else:\n",
    "        if type(group_id) == int:\n",
    "            group_dict[user_id] = [group_id]\n",
    "        else:\n",
    "             group_dict[user_id] = list(group_id)\n",
    "\n",
    "# Add group labels to the nodes\n",
    "for user_id, groups in group_dict.items():\n",
    "    nx.set_node_attributes(G_reddit, {user_id: groups}, 'group_belonging')\n",
    "\n",
    "# Print basic graph information\n",
    "print(\"Number of nodes:\", G_reddit.number_of_nodes(), ' | ', \"Number of edges:\", G_reddit.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T09:54:34.238956Z",
     "start_time": "2024-01-06T09:17:46.294647Z"
    }
   },
   "id": "8ff30eafe708de6d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Find and preprocess labels for the graph\n",
    "labels = []\n",
    "for n in G_reddit.nodes:\n",
    "    l = G_reddit.nodes[n].get('group_belonging')\n",
    "    labels.append(l)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "preprocessed_labels = mlb.fit_transform(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T01:04:04.693171Z",
     "start_time": "2024-01-03T01:04:03.184019Z"
    }
   },
   "id": "ac961c5631a123b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# hyperparameters from the paper\n",
    "dim = 128\n",
    "alpha = 0.025\n",
    "neg_samples = 5\n",
    "T = 1e9\n",
    "\n",
    "model_reddit = Line(G_reddit, dim, alpha, neg_samples, T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T01:11:30.426217Z",
     "start_time": "2024-01-03T01:04:03.241378Z"
    }
   },
   "id": "4234f856c50c92b7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 502606923/1000000000 [10:37:36<10:30:59, 13137.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m embeddings_model_reddit \u001B[38;5;241m=\u001B[39m model_reddit\u001B[38;5;241m.\u001B[39mtrain_line(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreddit\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 69\u001B[0m, in \u001B[0;36mLine.train_line\u001B[0;34m(self, file_name)\u001B[0m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneg_samples):\n\u001B[1;32m     68\u001B[0m         v \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneg_table)\n\u001B[0;32m---> 69\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings[u], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings[v], alpha, acc_gradient, local_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings[u] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m acc_gradient\n\u001B[1;32m     73\u001B[0m norm_trained_embeddings \u001B[38;5;241m=\u001B[39m normalize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings)\n",
      "Cell \u001B[0;32mIn[3], line 33\u001B[0m, in \u001B[0;36mLine.update\u001B[0;34m(self, u_i, u_j, alpha, acc_gradient, local_label)\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m=\u001B[39m num_edge_samples\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nodes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim)) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, u_i, u_j, alpha, acc_gradient, local_label):\n\u001B[1;32m     34\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m    The update equation for the SGD in the LINE-1 algorithm\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m    :param u_i: source representation for vertex i\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    :return: \u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     f_line \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39m(u_i \u001B[38;5;241m@\u001B[39m u_j)))  \u001B[38;5;66;03m# sigmoid\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_model_reddit = model_reddit.train_line('reddit')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T11:49:43.082111Z",
     "start_time": "2024-01-03T01:11:27.939941Z"
    }
   },
   "id": "25850beb104d57a1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "np.save(f'./embeddings/reddit.npy', normalize(model_reddit.embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T11:50:59.839010Z",
     "start_time": "2024-01-03T11:50:59.001426Z"
    }
   },
   "id": "d264958fc3aaaaa5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings_model_reddit = np.load('./embeddings/reddit.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T10:09:27.496852Z",
     "start_time": "2024-01-06T10:09:27.310399Z"
    }
   },
   "id": "2a9f72f60914c1b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Node Classification:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b294541575ad0f1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- F1 macro: 0.8872435748014531 F1 micro: 0.9232672579502028\n",
      "Shuffle 2 -- F1 macro: 0.88720976200274 F1 micro: 0.923153071842032\n",
      "Shuffle 3 -- F1 macro: 0.8872037583157116 F1 micro: 0.9232989911232746\n",
      "Shuffle 4 -- F1 macro: 0.887149947471468 F1 micro: 0.9232640658000217\n",
      "Shuffle 5 -- F1 macro: 0.8873171961993054 F1 micro: 0.9232863279466331\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "f1_macros, f1_micros = np.zeros(N), np.zeros(N)\n",
    "for i in range(N):\n",
    "    _, f1_macro, f1_micro = node_classification(embeddings_model_reddit, preprocessed_labels)\n",
    "    f1_macros[i] = f1_macro\n",
    "    f1_micros[i] = f1_micro\n",
    "    print(f'Shuffle {i+1} --', 'F1 macro:', f1_macro, 'F1 micro:', f1_micro)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:16:40.602574Z",
     "start_time": "2024-01-03T11:51:15.625158Z"
    }
   },
   "id": "cfb83601282132eb"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Final results: -----\n",
      "F1 macro: 0.8872248477581357 F1 micro: 0.923253942932433\n"
     ]
    }
   ],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('F1 macro:', np.mean(f1_macros), 'F1 micro:', np.mean(f1_micros))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T12:16:40.654781Z",
     "start_time": "2024-01-03T12:16:40.621418Z"
    }
   },
   "id": "8796b3d4af96d956"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Link Prediction:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afbb83a57b97bf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle 1 -- ROC-AUC: 0.9143625631022497\n",
      "Shuffle 2 -- ROC-AUC: 0.9142920870122443\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "roc_auc_scores = np.zeros(N)\n",
    "for i in range(N):\n",
    "    train_data, test_data = train_test_split_graph(G_reddit)\n",
    "\n",
    "    # Prepare edges\n",
    "    test_edges = test_data.edge_label_index.numpy().T\n",
    "    y_true = test_data.edge_label.numpy()\n",
    "\n",
    "    score = link_predictions(embeddings_model_reddit, test_edges, y_true)\n",
    "    roc_auc_scores[i] = score\n",
    "    print(f'Shuffle {i+1} --', 'ROC-AUC:', score)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-06T10:09:35.739181Z"
    }
   },
   "id": "623399c247fd7686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('-----  Final results: -----')\n",
    "print('ROC-AUC:', np.mean(roc_auc_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "60d0846478136a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e8b41dc58560aad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
